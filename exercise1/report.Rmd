---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 1: Group 13"
author: "Øyvind Klåpbakken, Martin Outzen Berild and Sindre Henriksen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  #pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "hold")
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
```

# Problem 1: Multiple linear regression 

```{r,echo=TRUE,eval=TRUE}
library(GLMsData)
data("lungcap")
lungcap$Htcm=lungcap$Ht*2.54
modelA = lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelA)
```

**Q1:** 
The equation for the fitted 'modelA' is
$$\log(FEV) = -1.944 + +.0234 \times Age + 0.0168 \times Htcm + 0.0293 \times 
GenderM - 0.0460 \times Smoke \, ,$$
where `GenderM` is 0 if gender is female and 1 if gender is male. The true
model is assumed to be
$$\log(FEV) = \beta_0 + \beta_1 \times Age + \beta_2 \times Htcm + \beta_3
\times GenderM + \beta_4 \times Smoke + \epsilon \, ,$$
where $\epsilon \sim \mathcal{N}(0, \sigma^2)$.

**Q2:** 
    
* `Estimate` values are the estimates of the coefficients 
  $\hat{\beta}=(X^\intercal X)^{-1}X^\intercal Y$. The intercept 
  estimate is the value of `log(FEV)` if all the
  covariates are 0. For the covavariates the `Estimate` values say how much
  `log(FEV)` increases with one covariate if all the other covariates are kept
  constant (according
  to the model). The model point estimate of `log(FEV)` for given covariate
  values is found by using the formula in Q1. So if e.g. `Age` is increased by
  1 and all the other covariates are kept constant, the estimate of `log(FEV)`
  increases by 0.0234.
* `Std.Error` is the standard error of the estimated coefficients (the values
  under `Estimate`). This tells how much the estimates will vary if we repeat
  the experiment with the same covariate values if our model assumptions are
  true. The values are the diagonal elements of the coefficient estimate
  covariance matrix $Cov(\hat{\beta})=(X^\intercal X)^{-1}\sigma^2$.
* `Residual standard error` is an unbiased estimator for the standard deviation
$\sigma$ of the error term $\epsilon$ in our model $y = \beta^\intercal x +
  \epsilon$, where $\beta$ is the vector of coefficients and $x_i = (1, Age_i, 
  Htcm_i, GenderM_i, Smoke_i)$. It is given by
  $$RSE = \hat{\sigma} = \frac{RSS}{n-p-1} \, , \quad \text{where} \quad RSS = 
  \sum_{i=1}^n(Y_i-\hat{Y})^2$$
  and n is the number of observations and p the number of covariats (4).
* `F-statistic` is a statistic for the null hypothesis 
  $H_0:\beta_1=\beta_2=\beta_3=\beta_4=0$ vs the alternative hypothesis $H_1$:
  not all the coefficients (except the intercept) are 0. The p-value is very
  small, so in this case 
  $H_0$ is clearly rejected, i.e. the regression is significant. The value and
  the p-value is given by
  $$F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)} \sim F_{p, n-p-1}\, , \quad
  TSS = \sum_{i=1}^n(Y_i-\bar{Y})^2 \, .$$
$p$ and $n-p-1$ are the degrees of freedom of the F-distribution.

**Q3:** 
Multiple R-squared is the proportion of the variability in the response variable
explained by the model, i.e.
$$\frac{TSS-RSS}{TSS} \, .$$
So here 81.06% of the variability is explained by the model.

**Q4:**

```{r,eval=TRUE}
library(ggplot2)
# residuals vs fitted
ggplot(modelA, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals",
       subtitle = deparse(modelA$call))

# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q", subtitle = deparse(modelA$call))

# normality test
library(nortest) 
ad.test(rstudent(modelA))
```

The plot of standardized residuals shows that there is no clear trend, i.e. the
residuals seem to be randomly distributed around 0 for all fitted values. We
also can't see any covariance structure from this plot. So this plot does not
indicate that any of our model assumptions are incorrect.

The normal QQ-plot, however, shows that the standardized residuals are not 
perfectly normally distributed. This is confirmed by the low p-value from the
Anderson-Darling normality test, leading to rejection of the null hypothesis
that the residuals are normally distributed.

**Q5:** 

```{r,eval=TRUE}
modelB = lm(FEV ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelB)

# residuals vs fitted
ggplot(modelB, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Standardized residuals",
       title = "Fitted values vs. Standardized residuals",
       subtitle = deparse(modelB$call))

# qq-plot of residuals
ggplot(modelB, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", 
       title = "Normal Q-Q", subtitle = deparse(modelB$call))

# normality test
library(nortest) 
ad.test(rstudent(modelB))
```
The model with `FEV` as response instead of `log(FEV)` is fitted an testet in
the code above. We see that the proportion of variability explained is somewhat
smaller than before (77.54%) (we don't need to look at adjusted R-squared since 
the number of covariates in the models are the same). Also, the standardized
residuals do not seem to
be distributed around 0 for all fitted values. The studentized residuals are
somewhat more likely to be normally distributed, but still get a p-value as low
as 0.4% from the Anderson-Darling test. Notable is the fact that `Smoke` is no 
longer considered to have a significant explanatory power, which makes room for
a more parsimonous model. But then again, smoking probably does affect lung 
capacity. All in all `modelA` is preferable, since our
model assumptions seem more reasonable for `log(FEV)` as response.

**Q6:** 
We wish to test $H_0: \beta_{Age} = 0$ against $H_1: \beta_{Age} \neq 0$. We
use the test statistic
$$T_{Age} = \frac{\hat{\beta}_{Age}}{\sqrt{c_{jj}}\hat{\sigma}} \sim t_{n-p-1}\,
,$$
where $c_{jj}$ is the second diagonal element of $X^\intercal X)^{-1}$. The
calculations are already performed by `lm` and printed in the summary before.
The t-value is 6.984, resulting in a p-value of 7.1e-12, so the null hypothesis
is rejected at any reasonable signifiance level, i.e. `Age` has a significant 
effect on the response. Common siginficance levels are 0.05 and 0.01.

**Q7:** 

```{r,eval=TRUE}
beta_age = modelA$coefficients[2][["Age"]]
alpha = 0.01
p = 4
n = nrow(lungcap)
t = qt(alpha/2, n-p-1)
se_beta_age = sqrt(vcov(modelA)[2,2])
CI99 = c(beta_age + t*se_beta_age, beta_age - t*se_beta_age)
CI99
```

A 99% confidence interval for $\beta_{Age}$ is found and printed above, using
$$ P(\hat{\beta}_j-t_{\alpha/2,n-p-1}\sqrt{c_{jj}}\hat{\sigma}
\le \beta_j \le \hat{\beta}_j+t_{\alpha/2,n-p-1}\sqrt{c_{jj}}\hat{\sigma})=
1-\alpha \, ,$$
where $\sqrt{c_{jj}}\hat{\sigma} = SE(\hat{beta_{Age}})$. This interval tells us
that when repeating the experiment and measuring the
response over and over, 99% of such intervals will contain the true value
$\beta_{age}$. Since the interval does not contain the value 0, we can say for
sure that the p-value for the test in Q6 is less than 1%.


**Q8:**

```{r,eval=TRUE}
new = data.frame(Age=16, Htcm=170, Gender="M", Smoke=0)
log_prediction = predict.lm(modelA, new, interval="prediction", level=0.95)
log_prediction
prediction = exp(log_prediction)
prediction
```

Our best guess for the value of `log(FEV)` for new person is
`r format(round(log_prediction[1], 3), nsmall=3)`. A 95% prediction interval for
his `FEV` is (`r format(round(prediction[2], 3), nsmall=3)`,
`r format(round(prediction[3], 3), nsmall=3)`). This tells us that we can say
with 95% certainty (given that our model assumptions are true) that he has a
relatively high lung capacity; possibly close to the sample maximum
`r max(lungcap["FEV"])` and likely not close to the minimum
`r min(lungcap["FEV"])`. The interval is, however, quite big.

# Problem 2: Classification 

```{r}
library(class)# for function knn
library(caret)# for confusion matrices

raw = read.csv("https://www.math.ntnu.no/emner/TMA4268/2019v/data/tennis.csv")
M = na.omit(data.frame(y=as.factor(raw$Result),
                       x1=raw$ACE.1-raw$UFE.1-raw$DBF.1, 
                       x2=raw$ACE.2-raw$UFE.2-raw$DBF.2))
set.seed(4268) # for reproducibility
tr = sample.int(nrow(M),nrow(M)/2)
trte=rep(1,nrow(M))
trte[tr]=0
Mdf=data.frame(M,"istest"=as.factor(trte))
```

**Q9:** 
$\hat{y} = \hat{R}(Y = j|X = x_0) = \frac{1}{K}\sum\limits_{i\in\mathcal{N}_0}I(y_i = j)$

**Q10:** 
```{r,eval=TRUE}
ks = 1:30
train.var = Mdf[Mdf$istest == 0, 2:3]
test.var = Mdf[Mdf$istest == 1, 2:3]
train.def = Mdf[Mdf$istest == 0, 1]
test.def = Mdf[Mdf$istest == 1, 1]
train.e = numeric(30)
test.e = numeric(30)
for (k in ks){
  model1 <- class::knn(train = train.var, test = test.var, cl = train.def, k = k, prob=TRUE)
  model2 <- class::knn(train = train.var, test = train.var, cl = train.def, k = k, prob=TRUE)
  train.e[k] = mean(train.def != model2)
  test.e[k] = mean(test.def != model1)
}
train.e
test.e
```


```{r, eval=TRUE}
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop. 
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv", 
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){ 
  sapply(seq_along(idx), function(j) {
    yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
               cl=M[tr[ -idx[[j]] ], 1],
               test=M[tr[ idx[[j]] ], -1], k = k)
    mean(M[tr[ idx[[j]] ], 1] != yhat)
  })
})
```


**Q11:** 

```{r, eval=TRUE}
cv.e = colMeans(cv)
cv.se = apply(cv,2,sd)/sqrt(5)
k.min = which.min(cv.e)
```

**Q12:** 

```{r,eval=TRUE}
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
     xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
```

**Q13:** 
The strategy used in the first line is the \textit{one standard error rule}, which means that we choose the highest K for which $CV(\theta) \leq CV(\hat{\theta}) + SE(\hat{\theta})$, where $\hat{\theta}$ is k.min from **Q12**
```{r,eval=FALSE}
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5, 
        xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
        main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"), 
       col=c("red", "black"), pch=1)
box()
```


**Q14:** 
The optimal choice is $K = 30$ from **Q13**. 
```{r,eval=TRUE}
K=30
  
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set

library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1] 
# in your call to the function roc in the pROC library
myRoc <- roc(response = M[-tr,1], predictor = KNNprob, auc = TRUE, ci = TRUE)
interval = 0.25
breaks = seq(0,1,interval)
ggplot(NULL, aes(x =rev(myRoc$specificities), y =rev(myRoc$sensitivities)))+
  geom_segment(aes(x = 0, y = 1, xend = 1,yend = 0), linetype = "dashed", alpha = 0.5) + 
  geom_step() +
  scale_x_reverse(name = "Specificity",limits = c(1,0), breaks = breaks, expand = c(0.001,0.001)) + 
  scale_y_continuous(name = "Sensitivity", limits = c(0,1), breaks = breaks, expand = c(0.001, 0.001)) +
  theme_bw() + 
  theme(axis.ticks = element_line(color = "grey80")) +
  coord_equal() + 
  annotate("text", x = interval/2, y = interval/2, vjust = 0, label = paste("AUC =",sprintf("%.3f",myRoc$auc)))
```

**Q15:**
#The argument which creates the lowest cv error
```{r,eval=TRUE}
  quality_score <- data.frame(Mdf, y_hat = as.integer(M$x1>M$x2))

  ggplot(quality_score)+
    geom_point(aes(x = x1, y= x2, color = y,shape=istest))+
    #geom_area(aes(c(-100,20),c(-100,20)),alpha=0.1,fill="pink3")+
    geom_abline(aes(slope=1,intercept=0))
```


# Problem 3: Bias-variance trade-off 

Here you see how to write formulas with latex (needed below)
$$
\hat{\boldsymbol \beta}=({\bf X}^T{\bf X})^{-1}{\bf X}^T{\bf Y}
$$

**Q16:**

Utilizing that $\text{E}(Y) = X\beta$ and $\text{E}(CX) = C\text{E}(X)$ we obtain that $\hat{\beta}$ is an unbiased estimator for $\beta$.

$$
\text{E}(\hat{\beta}) = \text{E}\left[(X^TX)^{-1}X^T Y\right]  \\
= (X^TX)^{-1}X^T\text{E}(Y) = (X^TX)^{-1}X^TX\beta \\
= \beta
$$
By using that $\text{Cov}(CY) = C\text{Cov}(Y)C^T$ 
we obtain 

$$
\text{Cov}(\hat{\beta}) = \text{Cov}\left[(X^TX)^{-1}X^TY\right] \\ 
= (X^TX)^{-1}X^T\text{Cov}(Y)\left[(X^TX)^{-1}X^T\right]^T \\
= \sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1} \\ 
= \sigma^2(X^TX)^{-1}
$$
**Q17:** 

Using the same tools as above, we obtain

$$
\text{E}\left[\hat{f}(\bf{x}_0^T)\right] = \text{E}(\bf{x}_0^T\hat{\beta}) = \bf{x}_0^T\text{E}(\hat{\beta}) = x_0^T\beta
$$
$$
\text{Var}\left[\hat{f}(\bf{x}_0^T)\right] = \text{Var}(\bf{x}_0^T\hat{\beta})
= \bf{x}_0^T\text{Var}(Y)\bf{x}_0 \\ 
= \bf{x}_0^T\sigma^2(X^TX)^{-1}\bf{x}_0
$$

**Q18:** 
$$\text{E}[(Y_0-\hat{f}({\bf x}_0))^2]=[\text{E}(\hat{f}({\bf x}_0)-f({\bf x}_0)]^2+\text{Var}(\hat{f}({\bf x}_0) ) + \text{Var}(\varepsilon)$$

We start off by noting that $\text{E}(f(\bf{x}_0)) = \text{E}(Y_0 - \epsilon) = \text{E}(\bf{x}_0^T\beta)$, which enables us to write $(\bf{x}_0^T - x_0^T\hat{\beta})^2$ as $\text{E}(f(\bf{x}_0) - \hat{f}(\bf{x}_0))^2$. The expression for $\text{E}\left[(Y_0 - \hat{f}(x_0))^2\right]$ becomes

$$
\text{E}\left[(Y_0 - \hat{f}(x_0))^2\right] =
\text{Var}(Y_0 - \hat{f}(x_0)) + \text{E}(Y_0 - \hat{f}(\bf{x}_0))^2 \\
= \text{Var}(\bf{x}_0^T\beta + \epsilon + \bf{x}_0^T\hat{\beta}) \\ + \text{E}(\bf{x}_0^T\beta + \epsilon + \bf{x}_0^T\hat{\beta})^2 \\
= \text{Var}(\epsilon) + \text{Var}(\hat{f}(\bf{x}_0)) + \left(\text{E}\left[f(\bf{x}_0) - \hat{f}(\bf{x}_0)\right]\right)^2
$$



Ridge estimator:
$$
\widetilde{\boldsymbol \beta}=({\bf X}^T{\bf X}+\lambda {\bf I})^{-1}{\bf X}^T{\bf Y}
$$

**Q19:**

We start off by defining $G := X^TX$, and by rewriting $\hat{\beta}$ as $\hat{\beta} = G^{-1}X^TY$. Note that G is a symmetric matrix. We can now write $\tilde{\beta}$ as 

$$
\tilde{\beta} = (G - {\lambda}I)^{-1}X^TY = (G - {\lambda}I)^{-1}GG^{-1}X^TY = (G + {\lambda}I)^{-1}G\hat{\beta}
$$
$$
= \left[G(I + {\lambda}G^{-1})\right]^{-1}G\hat{\beta} = (I + {\lambda}G^{-1})^{-1}G^{-1}G\hat{\beta} = (I + {\lambda}G^{-1})^{-1}\hat{\beta}
$$
By defining $H := (I + {\lambda}G^{-1})$, we end up with

$$
\tilde{\beta} = H^{-1}\hat{\beta}
$$
It follows from the previous results, as well as the properties introduced earlier that
$$
\text{E}(\tilde{\beta}) = H^{-1}\text{E}(\hat{\beta}) = H^{-1}\beta
$$
and

$$
\text{Cov}(\tilde{\beta}) = H^{-1}\text{Cov}(\hat{\beta})(H^{-1})^T = \sigma^2(HGH)^{-1}
$$

**Q20:** 

$$
\text{E}(\bf{x}_0^T\tilde{\beta}) = \bf{x}_0^TH^{-1}\beta
$$
$$
\text{Var}(\bf{x}_0^T\tilde{\beta}) = \sigma^2\bf{x}_0^T(HGH)^{-1}\bf{x}_0
$$

**Q21:** 
$$\text{E}[(Y_0-\widetilde{f}({\bf x}_0))^2]=[\text{E}(\widetilde{f}({\bf x}_0)-f({\bf x}_0)]^2+\text{Var}(\widetilde{f}({\bf x}_0) ) + \text{Var}(\varepsilon)$$
Using that $\text{Var}(X) = \text{E}(X^2) - \text{E}(X)^2$, we obtain

$$
\text{E}[(Y_0-\tilde{f}(\bf{x}_0))^2] = \text{Var}(\bf{x}_0^T\beta + \epsilon + \bf{x}_0^T\tilde{\beta}) + \left[\text{E}(\bf{x}_0^T\beta + \epsilon - \bf{x}_0^T\hat{\beta})\right]^2
$$
$$
=\text{Var}(\epsilon) + \text{Var}(\tilde{f}(\bf{x_0})) + \left[\text{E}(f(\bf{x_0}) - \tilde{f}(\bf{x}_0^T) \right]^2
$$


```{r}
values=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X=values$X
dim(X)
x0=values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
```

Hint: we perform matrix multiplication using `%*%`, transpose of a matrix `A` with `t(A)` and inverse with `solve(A)`. 

**Q22:** 

The squared bias is zero for $\lambda = 0$ as one would expect from the results above. The fact that the error is approaching zero again around $\lambda = 0.5$ is more unexpected and. It then proceeds in an expected way by increasing as $lambda$ increases. 

```{r,eval=TRUE}
sqbias=function(lambda,X,x0,beta)
{
  p=dim(X)[2]
  G <- t(X)%*%X
  H <- (diag(p) + lambda*solve(G))
  value <- (t(x0)%*%beta - t(x0)%*%solve(H)%*%beta)^2 
  return(value)
}

thislambda=seq(0,2,length=500)
sqbiaslambda=rep(NA,length(thislambda))
for (i in 1:length(thislambda)) sqbiaslambda[i]=sqbias(thislambda[i],X,x0,beta)
plot(thislambda,sqbiaslambda,col=2,type="l")
```

**Q23:** 

Having seen the previous plot, and by remembering that the expected squared prediction error is made of up of the sum of the variance and the squared bias we would expect to see that the variance drops as the squared bias increases. Therefore, the plot of the variance is not surprising.

```{r,eval=TRUE}
variance=function(lambda,X,x0,sigma)
{
  p=dim(X)[2]
  inv=solve(t(X)%*%X+lambda*diag(p))
  G <- t(X)%*%X
  H <- (diag(p) + lambda*solve(G))
  value <- sigma^2*t(x0)%*%solve(H%*%G%*%H)%*%x0
  return(value)
}
thislambda=seq(0,2,length=500)
variancelambda=rep(NA,length(thislambda))
for (i in 1:length(thislambda)) variancelambda[i]=variance(thislambda[i],X,x0,sigma)
plot(thislambda,variancelambda,col=4,type="l")
```


**Q24:** 

```{r,eval=TRUE}
tot=sqbiaslambda+variancelambda+sigma^2
which.min(tot)
thislambda[which.min(tot)]
plot(thislambda,tot,col=1,type="l",ylim=c(0,max(tot)))
lines(thislambda, sqbiaslambda,col=2)
lines(thislambda, variancelambda,col=4)
lines(thislambda,rep(sigma^2,500),col="orange")
abline(v=thislambda[which.min(tot)],col=3)
```
The optimal value for $\lambda$ is shown in the table below. An alternative approach to plotting and finding this optimal value is presented below. It serves the same purpose as what is done above.

```{r,eval=TRUE}
library(tidyverse)
lambdas <- seq(0, 2, length = 500)
df <- tibble(lambda = lambdas,
             sqbias = lambdas %>%
               map_dbl(partial(sqbias, x0 = x0, beta = beta, X = X)),
             variance = lambdas %>%
               map_dbl(partial(variance, x0 = x0, sigma = sigma, X = X)))
df <- df %>% mutate(tot = sqbias + variance + sigma^2)
min <- df %>% filter(tot == min(tot)) %>% select(lambda, tot)
df %>% ggplot(aes(x = lambda)) +
  geom_line(aes(y = variance, color = 'Variance')) +
  geom_line(aes(y = sqbias, color = 'Squared bias')) +
  geom_line(aes(y = tot, color = 'Expected squared error')) +
  geom_hline(aes(color = 'Irreducible error', yintercept = sigma^2)) + 
  geom_vline(aes(xintercept = min %>% select(lambda) %>% as.numeric())) +
  xlab(expression(lambda)) + 
  ylab(NULL) + 
  ggtitle('Components of expected squared error') +
  theme(legend.title = element_blank())

min %>% knitr::kable(caption = 'Optimal expected squared error',
              col.names = c('$\\lambda$', 'Expected squared error'),
              escape=FALSE)

```



